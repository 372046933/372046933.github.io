---
comments: true
---

# 对于线性回归的理解
线性回归假设output是由feature的线性组合得到的，因为feature本身可以cross，所以线性回归能表
征的关系不一定就那么简单的直来直去（想想Taylor's Expansion可以表达的函数种类）

常见的损失函数为平方损失，即$$J(\theta) = \frac{1}{2} \sum_i^m(\theta^T x^{(i)} - y^i)^2 $$，为什么一定时平方损失
，这个在后面的[概率解释](#probabilistic-interpretation)中会提到
## 梯度下降法
$$ 
\begin{align*}
\frac{\partial J}{\partial \theta} &= 
	\sum_i^m (\theta^T x - y) \frac{\partial}{\partial \theta} (\theta^T x - y)\\
	&= \sum_i^m (\theta^T x - y) x
\end{align*}
$$

通常使用stochastic gradient或mini-batch就能收敛了



## [概率解释](#probabilistic-interpretation)
假设 $$y^i = \theta^Tx^{(i)} + \epsilon^{i}, \epsilon^{i} \sim N(0, \sigma^2)$$,
$$\sigma^i$$的概率密度函数
$$p(\epsilon^i) = \frac{1}{\sqrt{2\pi} \sigma} \mbox{exp}(\frac{(\sigma^{i})^2}{2\sigma^2})$$，
进而$$y^i$$的概率密度函数为
$$p(y^i| x^{(i)} ; \theta) = \frac{1}{\sqrt{2\pi} \sigma} \mbox{exp}(\frac{(y^i - \theta^T x^{(i)})^2}{2\sigma^2})$$，
利用最大似然估计，得到的结果是需要最小化$$\frac{1}{2} \sum_i^m(\theta^T x^{(i)} - y^i)^2$$，这也是为啥损失函数是平方损失的原因


## Normal Equation
损失函数可以用矩阵形式写成 $$J(\theta) = \frac{1}{2}(X\theta - y)^T(X\theta -y)$$

$$
\begin{align*}
\nabla_\theta (J(\theta))
	&= \nabla_\theta \frac{1}{2} (X\theta - y)^T (X\theta -y)\\
	&= \nabla_\theta \frac{1}{2} (\theta^TX^TX\theta - \theta^TX^Ty - y^TX\theta + y^Ty)\\
	&= \nabla_\theta \frac{1}{2} \mbox{tr} (\theta^TX^TX\theta - \theta^TX^Ty - y^TX\theta + y^Ty)\\
	&= \nabla_\theta \frac{1}{2} (\mbox{tr} \theta^TX^TX\theta - 2\mbox{tr} y^TX\theta)\\
	&= \frac{1}{2} (X^TX\theta + X^TX\theta - 2 X^Ty)\\
	&= X^TX\theta - X^Ty
\end{align*}
$$
令$$\nabla_\theta (J(\theta)) = 0$$, 得到 $$\theta = (X^TX)^{-1}X^Ty$$

另一个思路是，需要找到权重向量$$\theta$$，使得方程组$$X\theta = y$$成立。这个形式有很好的
几何含义，即寻找解$$\theta$$，使得$$y$$尽量落在$$X$$的column space中;即寻找$$y$$在
$$Col(X)$$中的投影。由[Projection subspace](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/projections-onto-subspaces/MIT18_06SCF11_Ses2.2sum.pdf)
知道，这个$$\theta = (X^TX)^{-1}X^Ty$$
